# 七並べAI 最強設計案：PIMC法 (Perfect Information Monte Carlo)

## 1. アプローチの概要：なぜこれが「最強」なのか
「適応型ルールベース」は安定していますが、「想定外の状況」に弱く、相手の手札分布を考慮した「読み」ができません。
一方、ここで提案する**PIMC法**は、不完全情報ゲーム（ポーカー、ブリッジ等）のAIで標準的に使われる「最強クラス」のアルゴリズムです。

**特徴:**
*   **学習不要**: 重いモデルファイルは不要。
*   **「読み」の実装**: 「相手がパスをした」＝「あのカードを持っていない」という情報を論理的に積み重ね、相手の手札を丸裸にします。
*   **並行世界シミュレーション**: 相手の手札の可能性を複数パターン（世界）生成し、それぞれの世界でゲームを解くことで、統計的に負けない手を打ちます。

---

## 2. アーキテクチャ構成

### Phase 1: Inference Engine (手札推論器 -「名探偵」)
ゲーム中、常に相手の手札の確率分布テーブル(`Belief State`)を更新し続けます。

*   **初期状態**: 自分以外のカードは、誰が持っているか均等確率。
*   **確定情報**: 場に出たカード、自分のカードは確率1.0/0.0で確定。
*   **推論ロジック (重要)**:
    *   **パス検知**: プレイヤーAが特定のスート・数字の場面でパスをした $\rightarrow$ Aはその時出せる候補カードを**持っていないことが確定**する。
    *   この「持っていない情報」を蓄積することで、終盤には相手の手札がほぼ特定できるようになります。

### Phase 2: Determinization (確定化 -「マルチバース生成」)
推論器の確率分布に基づき、見えないカードをランダムに配分して**「全員の手札が透けて見える仮想世界」**を $N$ 個（例: 50~100個）生成します。

*   World 1: 相手Aが ダイヤのKを持っている世界
*   World 2: 相手Bが ダイヤのKを持っている世界
*   ...

### Phase 3: Evaluation (評価 -「思考実験」)
各仮想世界において、ゲーム終了まで高速にプレイ（プレイアウト）します。

*   **自分のAI**: 最善手（または貪欲法）を選ぶ。
*   **相手のAI**: 合理的な手を選ぶと仮定（またはランダム）。
*   **集計**: すべての世界を通して、最も勝率が高かった（または得失点差が良かった）アクションを採用します。

---

## 3. 実装ロードマップ

### Step 1: 軽量シミュレータの再構築
Pythonはループが遅いため、現在の`State`クラスを極限まで軽量化する必要があります。
*   `numpy`の使用を最小限にする（または効果的に使う）。
*   オブジェクト生成コストを下げる。

### Step 2: 推論クラス (`CardTracker`) の実装
*   `possible_hands[player_id][card_id] = boolean` のようなテーブルを管理。
*   `observe_action(player, action, is_pass)` メソッドで情報を更新。

### Step 3: PIMC探索の実装
*   `my_AI` 関数内で、思考時間（例: 1秒）の許す限り仮想世界を生成・実行するループを作る。

---

## 4. 戦略的優位点
このAIは、相手が「パス」をして情報を漏らせば漏らすほど強くなります。ルールベースでは記述しきれない「このカードを出せば相手の選択肢が消える」といった複雑な詰み手順を、シミュレーションによって発見できます。

ファイルサイズも小さく、なおかつ「家で鍛えた脳みそ（DL）」に匹敵する、あるいは特定の状況下ではそれを凌駕する**「計算する知能」**となります。
