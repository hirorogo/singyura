# PIMC実装とチューニング履歴

作成日: 2026年1月16日

## 概要
七並べAIを `design_strongest.md` の方針に従い、PIMC (Perfect Information Monte Carlo) ベースで実装し、勝率最大化のためのチューニングを実施した記録。

---

## 実装済みの主要機能

### 1. 基盤実装
- **トンネルルール対応** (`State.legal_actions()`)
  - Aが出たスート → K側(8→K)のみ伸ばせる
  - Kが出たスート → A側(A→6)のみ伸ばせる
  - Colab notebook の競技ルールに準拠

- **ゲーム終了判定の修正** (`State.is_done()`)
  - バーストプレイヤーの空手札は勝者扱いしない
  - 残り1人以下で終了

- **行動履歴 (history)** の記録
  - 初期7配置から全行動を `(player, action, pass_flag)` で記録
  - 推論・リプレイで使用

### 2. Phase 1: 推論エンジン (CardTracker)
実装内容:
- `possible[p]` : プレイヤー p が持ちうるカードの集合（制約集合）
- **場に出たカード** → 全員が持たない
- **自分の手札** → 自分のみが持ちうる、他は持たない
- **パス観測** → その時点の `legal_actions()` のカードを持たないと判断
- **プレイ観測** → そのカードを全員が持たない

重要な改善:
- **履歴リプレイ機能** (`_build_tracker_from_history()`)
  - 履歴を先頭から逐次再生し、各手番時点の盤面で `legal_actions()` を評価
  - パス推論が「その時点の正しい盤面」で行われるように修正
  - トンネルルール下でも正確な推論が可能

### 3. Phase 2: 確定化 (Determinization)
実装内容:
- **制約付き確定化** (`_create_determinized_state_with_constraints()`)
  - `CardTracker.possible[p]` の制約を満たすように相手手札を割当
  - 30回リトライ、失敗時は制約なし確定化にフォールバック
  - 推論の効果を最大化

### 4. Phase 3: 評価 (Playout)
実装内容:
- **ロールアウトポリシー** (`_rollout_policy_action()`)
  - 端(A/K)優先 → Safe(次の札を自分が持つ)優先 → ランダム
  - PASSは基本しない（探索分散を避ける）
  - 再帰的PIMC呼び出しを回避するガード付き

- **AI同士想定の対応**
  - プレイアウト中も各プレイヤーが同じロールアウトポリシーで意思決定
  - `_in_simulation` フラグで再帰を防止

---

## チューニング経緯

### 試行1: 探索回数の増加
- `SIMULATION_COUNT: 30 → 200`
- `SIMULATION_DEPTH: 100 → 200`
- **結果**: 勝率 39% → 44% (vs ランダムAI)
- **速度**: 0.01s/game → 0.02s/game

### 試行2: 候補手フィルタの撤廃
**動機**: Safe moves 優先フィルタがトンネル環境下で勝ち筋を捨てている可能性

変更内容:
- `safe_moves` / `risky_moves` による絞り込みを撤廃
- 全合法手 + PASS を候補に入れてPIMCで評価

**結果**: 勝率 44% → 41% (悪化)
- PASSを候補に入れることで探索が分散
- トンネルルール下では「出せるカードがあるのにPASS」は損失が大きい

### 試行3: AI同士プレイアウトの試行と断念
**動機**: ベンチマーク相手もAI同士（同等の意思決定）を想定すべき

試行内容:
- プレイアウト中も各プレイヤーが `HybridStrongestAI` で意思決定

**問題**: 無限再帰
- PIMC → playout → PIMC → playout … で計算が終わらない
- `KeyboardInterrupt` で強制停止

**解決策**: ロールアウトポリシーの分離
- 本番意思決定: PIMC (推論 → 確定化 → 評価)
- プレイアウト: ロールアウトポリシー (軽量ヒューリスティック、再帰なし)
- `_in_simulation` フラグで切り替え

**最終結果**: 勝率 32% (vs ランダムAI)
- プレイアウトのモデル化が不十分（ランダムAI相手の最適戦略を学習していない）

---

## 現在の課題

### 1. 評価環境とプレイアウトモデルの不一致
- **ベンチマーク相手**: ランダムAI
- **プレイアウトモデル**: AI同士想定（端優先・Safe優先）

→ プレイアウトで学習する最適戦略が、実際の対戦相手（ランダム）と乖離している

### 2. 設計書との差分
現在の実装は `design_strongest.md` の方向性は満たしているが、以下が未実装:

- **確率分布 (Belief State)** ではなく制約集合（0/1）
  - 設計: `possible_hands[player_id][card_id] = probability`
  - 実装: `possible[p] = Set[Card]`
  - 重み付き決定化が未実装

- **戦略モード切替** (`strategy.md`)
  - `OpponentModel` は存在するが、評価への反映が弱い
  - Tunnel Lock / Burst Force モードの実装が不完全

### 3. 候補手の選択方針
- PASSを候補に入れると探索が分散して弱くなる
- Safe moves フィルタを外すと勝率が落ちる

→ 「合法手があるならPASSしない」が最も効果的な可能性

---

## 検討した戦略

### A. 相手モデルの推測（`strategy.md` より）
実装状況:
- `OpponentModel` クラスで簡易的に実装済み
- A/K早出し → aggressive フラグ
- パス頻度 → blocker フラグ
- モード判定: `tunnel_lock` / `burst_force` / `neutral`

未実装:
- モード別の評価関数の重み付け変更
- 動的ヒューリスティックの適用

### B. ロック戦略
- トンネルで相手のルートを封鎖
- 「次のカードを自分が持つ」＝ Safe move の考え方は実装済み
- ただし、トンネルルール下での詳細な封鎖ロジックは未実装

### C. バースト誘導
- 相手のパス回数を監視
- パスが多いプレイヤーが持っていないスートを急速に進める
- 現状では推論（CardTracker）はあるが、評価への反映が弱い

---

## 次のステップ候補（勝率最大化優先）

### 優先度: 高
1. **PASSを候補から除外**
   - 「合法手があるならPASSしない」に振り切る
   - 探索分散を防ぎ、バースト負けリスクを減らす

2. **プレイアウトモデルをベンチ相手に合わせる**
   - ランダムAI相手なら、プレイアウトもランダムにする
   - または、ベンチマーク相手を同じAIに変更して自己対戦で評価

3. **確定化の改善**
   - パス回数が多いプレイヤーほど「出しやすい札を持っていない」確率を上げる
   - 重み付き割当で推論の効果を強化

### 優先度: 中
4. **戦略モードの本格実装**
   - `OpponentModel.mode()` の結果を評価関数に反映
   - Tunnel Lock / Burst Force 時のスコア重み付け変更

5. **Belief State の確率化**
   - `possible[p]` を Set → Dict[Card, float] に変更
   - 確率分布に基づくサンプリング

### 優先度: 低（最適化）
6. **軽量化**
   - `State.clone()` の高速化

---

## ベンチマーク結果サマリ

| 試行 | SIMULATION_COUNT | フィルタ | プレイアウト | 勝率(P0) | 時間/game |
|------|------------------|----------|--------------|----------|-----------|
| 初期 | 30 | Safe優先 | 簡易ヒューリスティック | 39% | 0.01s |
| 探索増 | 200 | Safe優先 | 簡易ヒューリスティック | 44% | 0.02s |
| フィルタ撤廃 | 200 | なし(PASS含む) | 簡易ヒューリスティック | 41% | 0.01s |
| AI同士想定 | 200 | なし(PASS含む) | ロールアウトポリシー | 32% | 0.02s |

※ すべて vs ランダムAI（P1, P2）、100試合

---

## 結論

現在の実装は **PIMC + 推論 + 制約付き確定化** の骨格は完成しており、`design_strongest.md` の方向性に概ね沿っている。

ただし:
- **評価環境（ベンチ相手）とプレイアウトモデルの不一致**が勝率低下の主因
- トンネルルール環境では「PASSを候補に入れる」戦略が裏目に出やすい
- 確率分布化・戦略モード切替など、設計書の「フル実装」には至っていない

**次の一手（即効性）**:
1. PASSを候補から除外（合法手があるなら必ず打つ）
2. プレイアウトをランダムに戻す、またはベンチをAI同士に変更
3. 確定化の重み付け改善

これらで勝率は 50-60% 程度まで上がる見込み。さらに強化するなら、戦略モードの本格実装と確率化が必要。

---

## 参考資料
- `doc/design_strongest.md` : PIMC全体設計
- `doc/strategy.md` : 相手モデルと戦略切替
- `doc/misc/colab_notebook.md` : トンネルルール詳細
