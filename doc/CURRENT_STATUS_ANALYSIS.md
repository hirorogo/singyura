# AI性能分析と現状報告

## 作成日
2026年1月18日

## エグゼクティブサマリー

### 現状
- **現在の勝率**: 31-40%（main_improved.py、SIMULATION_COUNT=200-300）
- **目標勝率**: 55-60%
- **ベースライン（main.py）**: 44%（SIMULATION_COUNT=500）
- **問題**: 「改善版」が実際には性能を**低下**させている

### 主な発見

#### 🔴 重大問題1: PIMC実装の構造的問題
現在の実装は「1つの確定化に対して全ての候補手を評価」している。これは正しいPIMCの実装である。
しかし、シミュレーション回数が少ない場合、統計的なサンプル数が不足し、正確な評価ができない。

#### 🔴 重大問題2: 過度な最適化による脆弱性
Phase 1, Phase 2の「改善」として実装された以下の機能が、実際には性能を低下させている：

1. **重み付け確定化（ENABLE_WEIGHTED_DETERMINIZATION）**
   - パス回数に基づく重み付けが不適切
   - 確定化の失敗率が増加
   - 制約が厳しすぎて、フォールバックに頼る頻度が高い

2. **戦略ボーナス（Phase 2改善）**
   - トンネルロック戦略とバースト誘導戦略
   - ボーナス値が過大で、シミュレーション結果を歪める
   - 実際のゲーム状況を反映しない評価になっている

3. **パス観測ロジック**
   - パス時に合法手を全て除外する実装
   - 戦略的パスの可能性を無視
   - 過度に制約を強めて確定化を困難にする

#### 🔴 重大問題3: シミュレーション回数の不足
- オリジナル版: SIMULATION_COUNT=500で44%達成
- 改善版: SIMULATION_COUNT=200-300で31-40%
- **問題**: 計算コストを削減しすぎて、統計的信頼性が失われている

### 根本原因

**「最適化のつもりが過剰最適化に」**

1. 複雑な戦略ロジックを追加したが、それらが相互に干渉
2. 推論制約を厳しくしすぎて、確定化が失敗しやすくなった
3. シミュレーション回数を減らしすぎて、統計的な信頼性が低下
4. 各「改善」を個別にテストせず、全て同時に実装した結果、何が効いているか不明

---

## 詳細分析

### 1. CardTrackerの推論ロジック

#### 現在の実装
```python
if is_pass:
    self.pass_counts[player] += 1
    legal = state.legal_actions()
    for c in legal:
        self.possible[player].discard(c)  # 全て除外
```

**問題点:**
- 戦略的パスの可能性を考慮していない
- パスした時点での合法手を**全て**除外するのは過激
- 結果として、`possible[player]`が空になりやすく、確定化が失敗する

**推奨:**
- パス観測時は「持っていない可能性が高い」程度の情報として扱う
- 完全除外ではなく、確率的な重み付けを使用
- または、パス回数が多い場合のみ除外を強化

### 2. 確定化（Determinization）の問題

#### 現在の実装
```python
for _ in range(30):  # 30回リトライ
    ...
    possible_list = [c for c in remain if c in tracker.possible[p]]
    if len(possible_list) < k:
        ok = False
        break
```

**問題点:**
- 30回のリトライで失敗すると、制約なし確定化（フォールバック）に落ちる
- `possible[p]`が厳しすぎるため、リトライが頻繁に失敗
- フォールバックでは推論情報が完全に失われる

**推奨:**
- リトライ回数を増やす（例: 100回）
- 段階的な制約緩和（最初は厳しく、失敗したら徐々に緩める）
- 部分的な制約違反を許容（ソフト制約）

### 3. 戦略ボーナスの問題

#### Phase 2改善の戦略ボーナス
```python
# トンネルロック: -15点（出さない方向）
# バースト誘導: +5点×パス回数
```

**問題点:**
- ボーナス値が大きすぎる（シミュレーション結果の±1点に対して-15点）
- シミュレーションベースの評価を完全に上書きしている
- 実質的に「ルールベースAI」になってしまっている

**推奨:**
- ボーナス値を大幅に削減（例: -2点、+1点程度）
- またはボーナスを完全に無効化して、純粋なPIMCで評価
- フラグで制御可能にして、効果を測定

### 4. シミュレーション回数の問題

#### 現在の設定
- main_improved.py: SIMULATION_COUNT=200-300
- main.py: SIMULATION_COUNT=500

**問題点:**
- 候補手が5個ある場合、200回×5手=1000回のプレイアウトが必要
- しかし実際には200回のシミュレーション内で全候補を評価するため、
  各候補は200回のみの評価となる
- 統計的に200サンプルでは不十分（信頼区間が広い）

**推奨:**
- SIMULATION_COUNTを500以上に設定
- または、候補数に応じて動的に調整（main.pyの実装を参考）

---

## ベンチマーク結果の詳細

### Test 1: SIMULATION_COUNT=300, すべてのフラグON
```
勝率: 39.0%
処理時間: 0.22秒/ゲーム
```

### Test 2: SIMULATION_COUNT=300, パス観測改善（部分除外）
```
勝率: 40.0%
処理時間: 0.18秒/ゲーム
```

### Test 3: SIMULATION_COUNT=200, オリジナルのパス観測
```
勝率: 31.0%
処理時間: 0.21秒/ゲーム
```

**観察:**
- シミュレーション回数を減らすと勝率が大幅に低下
- パス観測の改善は若干の効果あり（+1%）
- 全体的に期待値を大きく下回る

### 位置バイアスの検出
Test 3の結果:
```
P0（AI）: 31%
P1（Random）: 26%
P2（Random）: 43%  ← 異常に高い！
```

**分析:**
- P2の勝率が43%と異常に高い
- 位置的な有利さが存在する可能性
- または、AIの実装にバグがある可能性

---

## 推奨される改善戦略

### Phase 0: 問題の切り分け（最優先）

1. **オリジナル版のベンチマーク**
   - main.pyを様々なSIMULATION_COUNTで実行
   - 200, 300, 500での勝率を測定
   - ベースラインを確立

2. **Phase 1/2改善のON/OFF比較**
   - 各改善フラグを個別にON/OFFして測定
   - どの改善が効いているか、逆効果かを特定

3. **位置バイアスの検証**
   - プレイヤー位置をローテーションして測定
   - P0, P1, P2で公平な比較

### Phase 1: 基本に戻る（推奨）

1. **シンプルなPIMC実装に戻す**
   - 戦略ボーナスを無効化
   - 重み付け確定化を無効化
   - パス観測を保守的に（除外しない、または弱く除外）

2. **SIMULATION_COUNTを増やす**
   - 最低500に設定
   - 計算時間が許す限り増やす

3. **効果を測定**
   - この「シンプル版」でベースラインを確立
   - 44%を超えることを確認

### Phase 2: 慎重な改善

1. **1つずつ改善を追加**
   - まずPASS除外のみ
   - 次に重み付け確定化のみ
   - 各改善の効果を個別に測定

2. **パラメータチューニング**
   - 戦略ボーナスの値を慎重に調整
   - 確定化のリトライ回数を調整

3. **継続的な測定**
   - 各変更後に必ずベンチマーク実行
   - 改善効果を定量的に確認

---

## 結論

**現状**: 「改善版」は実際には性能を**低下**させている。

**原因**: 過剰な最適化と複雑性の増加により、PIMC法の本質的な強みが失われている。

**解決策**: 
1. **まずシンプルな実装に戻る**
2. **SIMULATION_COUNTを増やす**
3. **1つずつ慎重に改善を追加**
4. **各変更の効果を測定**

**優先度**:
1. 🔥 **最優先**: オリジナル版のベンチマークと問題の切り分け
2. ⭐ **重要**: シンプル版の実装と検証（目標44%）
3. 💡 **次のステップ**: 慎重な改善の追加（目標55-60%）

---

## 次のアクション

### 即座に実行すべきこと

1. **オリジナル版（main.py）の詳細ベンチマーク**
   ```bash
   cd src
   # SIMULATION_COUNT=200でテスト
   # SIMULATION_COUNT=500でテスト
   # 各設定で100ゲーム以上実行
   ```

2. **シンプル版AIの作成**
   - main.pyをベースに、Phase 2改善を全て無効化
   - SIMULATION_COUNT=500に設定
   - まずこれで44%を再現できるか確認

3. **問題の文書化**
   - 各テスト結果を記録
   - 何が効いて、何が逆効果かを明確化

### 今後の方針

**「最適化」よりも「安定性」を優先**

複雑な戦略ロジックよりも、シンプルで確実に動くPIMC実装の方が、
このゲームにおいては有効である可能性が高い。

**データ駆動の改善**

感覚や理論ではなく、実際のベンチマーク結果に基づいて改善を進める。

**段階的なアプローチ**

一度に全ての改善を実装するのではなく、1つずつ追加して効果を測定する。

---

## 付録: テスト実行コマンド

```bash
# オリジナル版のベンチマーク
cd /home/runner/work/singyura/singyura/src
python benchmark.py

# 改善版のベンチマーク
python benchmark_improved.py

# SIMULATION_COUNTを変更してテスト
# main_improved.py の SIMULATION_COUNT を編集
# 200, 300, 500 でそれぞれテスト
```

---

**作成者**: GitHub Copilot Coding Agent  
**日時**: 2026年1月18日  
**プロジェクト**: hirorogo/singyura
